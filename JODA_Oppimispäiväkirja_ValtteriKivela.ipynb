{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oppimispäiväkirja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensimmäinen Johdatus datatieteeseen -kurssin luento pidettiin 9.3.2021 ja osallistuin luennolle katsomalla luentotallenteen varsinaisen luennon jälkeen. Ensimmäisen luennon aiheina olivat tyypilliseen tapaan kurssin opiskelukäytännöt sekä johdanto kurssin aiheisiin. Luennon pääteemana oli tutstuminen siihen, mitä datatiede oikeastaan on ja miten sitä voidaan hyödyntää työelämässä ja toimi ikään kuin introna sille, mitä seuraavina viikkoina tullaan kurssilla opiskelemaan. Lisäksi sivuttiin hieman työvälineitä, joilla dataa voidaan käsitellä ja visualisoida luettavaan muotoon helposti. Visualisointia ja datan käsittelyä käsiteltiin lisää perjantain demoluennolla.\n",
    "\n",
    "Kurssilla käytettävistä datateiteen työvälineistä olisi voinut mielestäni olla aloitusluennolla enemmänkin asiaa, vaikka ne toki tulevat tutuksi kurssin edetessä. Esimerkiksi itselleni tuli hieman yllätyksenä luennon jälkeen erilaisiin työkaluihin tutustuessa, että Anaconda sisältää kattavasti kaikenlaisia datahallinnan työkaluja ja, että siihen sisältyy mm. kurssin luentopäiväkirjan alustaksi suositeltu Jupyter Notebook. Anacondan asentamisesta saa myös pisteitä harjoitustyön tekemiseen, eli kysessä on kuitenkin varsin olennainen osa kurssin suoritusta, minkä perusteella Anaconda voisi olla suositeltu asennus heti kurssin ensimmäisen luennon jälkeen.\n",
    "\n",
    "Luennon lopuksi käsiteltiin hieman opiskelijoiden odotuksia kurssista. Itse toivon oppivani kurssilla käsittelemään raakadataa sievempään muotoon, käyttämään datahallinnan työkaluja sekä saavani tietoa datatieteen jatko-opiskelusta. Urasuunnitelmani tähtäävät tällä hetkellä jonkinlaiseen datatieteen, analytiikan ja liiketoiminnan kehittämisen yhdistelmään, minkä vuoksi motivaationi datatieteen opiskelua kohtaan on melko korkealla.\n",
    "\n",
    "Johdantoluennon pohjalta loppuviikon JODA-opinnot painottuivat Anacondan asennukseen sekä sen osa-ohjelmiin ja ominaisuuksiin tutustumiseen. Oppimispäiväkirjan ohjeissa oli, että jokaiselta luentoviikolta tulee oppimispäiväkirjaan sisällyttää myös koodia, joten tämän viikon koodi on vain testaus sille, miten koodin lisääminen Jupyter-työkirjassa toimii.\n",
    "\n",
    "\n",
    "Viikon Top 5 oivallukset:\n",
    "1. Datatiede koostuu tilastotieteen ja ohjelmoinnin lisäksi kommunikaatiotaidosta sekä liiketoimintaosaamisesta.\n",
    "2. Ollakseen hyvä datatieteilijä, ei tarvitse olla kaikkien edellä mainittujen osa-alueiden erityisasiantuntija vaan datatiede on usein tiimityötä.\n",
    "3. Kurssilla käytettävistä työkaluista on mahdollista valita mieluisensa mutta Anaconda vaikuttaa monipuolisimmalta.\n",
    "4. Datatiedettä on tehty jo pitkään, mutta silti sille on valtavasti kysyntää sillä tietoa on maailmassa enemmän kuin koskaan.\n",
    "5. Vaikka dataa on paljon, ei sitä jaksa kukaan lukea. Datatiedettä tarvitaan datan sieventämiseen, mallintamiseen ja analysoimiseen niin, että siitä saa helposti selvää."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "Give a number: 20\n",
      "Give a second number: 40\n",
      "\n",
      "   first  second  total\n",
      "a     20      40     60\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(\"Hello World!\")\n",
    "\n",
    "x = int(input(\"Give a number: \"))\n",
    "y = int(input(\"Give a second number: \"))\n",
    "\n",
    "print()\n",
    "\n",
    "numbers = [(x, y, x+y)]\n",
    "df1 = pd.DataFrame(numbers, columns=[\"first\", \"second\", \"total\"], index=[\"a\"])\n",
    "\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurssin kakkosluennolla esiteltiin datatieteen prosessin kulku ja syvennyttiin tarkemmin sen ensimmäiseen vaiheeseen, valmisteluun. Valmisteluvaiheessa kerätään ja siivotaan dataa niin, että sen analysoiminen sujuu myöhemmin  datan keräämiseen, johon liittyen myös demoluennolla tarkasteltiin verkkoryömijän toimintaa ja sen avulla datan raapimista verkosta. Katsoin luennot jälkikäteen luentotallenteista ja luin Github-sivulla linkitetyt luentoon liittyvät artikkelit. Luentotallenteilla ei kuulunut osallistujien ääneen tekemät kysymykset, joten kehitysehdotuksena tulevia luentoja varten voisi ääneen kysytyt kysymykset toistaa niin ne kuuluisivat myös tallenteella.\n",
    "\n",
    "Ennen kuin datatieteen prosessi käynnistetään on hyvä tietää, miksi datatiedettä tehdään. Luennolla otettiin aiheeseen liiketaloudellinen näkökulma ja datatieteen hyödyllisyyttä tarkasteltiin liiketaloudellisin perustein. Riittävästi kerättyä dataa oikein jalostettuna ja analysoituna on yritysten johdossa äärimmäisen arvokasta, jotta oikeansuuntaisia päätöksiä ja arvioita liiketoiminnan tulevaisuudelle voidaan tehdä. Tässä prosessissa datainsinöörit vastaavat pääasiallisesti datan keräämisestä, siirtämisestä ja säilyttämisestä, sekä tietojärjestelmärakenteiden ylläpidosta ja kehittämisestä. Datatieteilijän työnkuvaksi jää päättää ja etsiä lähteitä hyödynnettävälle datalle, etsiä keinoja päästä dataan käsiksi sekä tehdä datasta olennaisia analyysejä ja johtopäätöksiä. ([ETL/DAD](https://www.datasciencecentral.com/profiles/blogs/data-scientist-versus-data-engineer)) \n",
    "\n",
    "Kun haluttua tietoa on löydetty internetistä, voidaan sen tallentamisessa hyödyntää verkkoryömijöitä ([Web Crawler](https://en.wikipedia.org/wiki/Web_crawler)) ja datan raapimista ([Web Scraping](https://en.wikipedia.org/wiki/Web_scraping)). Luennon aikana kyseiset termit eivät vielä minulle oikein auenneet, mutta perjantaina esitelty demo oli erinomaisen havainnollistava ja koen että ymmärrän nyt varsin hyvin näiden toimintaperiaatteet. Ryömijälle annetaan jokin nettisivu, jolta se aloittaa tietojen hakemisen, sekä paikka html-koodissa, mistä siltä pyydetty tieto löytyy. Näiden lisäksi ryömijälle kerrotaan, mistä se löytää linkin seuraavalle nettisivulle. Ryömijä etsii ja tallentaa aloitussivulta siltä pyydetyt tiedot, jonka jälkeen se siirtyy sille kerrotusta kohdasta löytyvästä linkistä seuraavalle sivulle etsimään lisää tietoja. Näin ryömijä ikäänkuin ryömii pitkin nettisivun linkkirakennetta ja etsii ja tallentaa siltä pyydettyjä tietoja niin kauan, kunnes se ei pääse enää eteenpäin.\n",
    "\n",
    "Lisää ryömijöistä ja datatieteestä oppii varmasti, kun pääsee toteuttamaan ja kokeilemaan, minkä vuoksi odotankin innolla jo seuraavia luentoja ja harjoitustyötä. Demoluennon esityksen perusteella olen myös kokeillut tämän viikon koodina raapia dataa verkosta. Mieleeni nousi myös ajatuksia, kuinka ryömijäbotista voisi kehittää paremman antamalla sille esimerkiksi valmiita hakusanoja tai muita syötteitä, joita se voisi nettisivuilla vieraillessaan hyödyntää.\n",
    "\n",
    "Viikon Top 5 oivallukset:\n",
    "1. Datan ja analytiikan hyödyntämiselle on liike-elämässä vahvat perustelut\n",
    "2. Mikä web crawler on ja miten se toimii\n",
    "3. Miten internetistä voi kerätä tietoa datan raavintaa hyödyntämällä\n",
    "4. Vaikka dataa on internetissä vapaasti saatavilla, sen systemaattinen kerääminen ja analysointi omia tarkoituksiaan varten ei välttämättä ole eettistä\n",
    "5. Tiedon analysoinnin kannalta on erittäin paljon merkitystä, että analysoitava data on yhteneväisessä muodossa, jolloin sitä voidaan helpommin käsitellä koneellisesti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.4.1-py2.py3-none-any.whl (239 kB)\n",
      "\u001b[K     |████████████████████████████████| 239 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting parsel>=1.5.0\n",
      "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in /Users/valtterikivela/opt/anaconda3/lib/python3.8/site-packages (from scrapy) (4.6.1)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /Users/valtterikivela/opt/anaconda3/lib/python3.8/site-packages (from scrapy) (19.1.0)\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting protego>=0.1.15\n",
      "  Downloading Protego-0.1.16.tar.gz (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=2.0 in /Users/valtterikivela/opt/anaconda3/lib/python3.8/site-packages (from scrapy) (3.1.1)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Downloading service_identity-18.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.5.tar.gz (34 kB)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in /Users/valtterikivela/opt/anaconda3/lib/python3.8/site-packages (from scrapy) (5.1.2)\n",
      "Collecting Twisted>=17.9.0\n",
      "  Downloading Twisted-21.2.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 756 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.5.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six>=1.6.0 in /Users/valtterikivela/opt/anaconda3/lib/python3.8/site-packages (from parsel>=1.5.0->scrapy) (1.15.0)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /Users/valtterikivela/opt/anaconda3/lib/python3.8/site-packages (from cryptography>=2.0->scrapy) (1.14.3)\n",
      "Collecting pyasn1\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=16.0.0 in /Users/valtterikivela/opt/anaconda3/lib/python3.8/site-packages (from service-identity>=16.0.0->scrapy) (20.3.0)\n",
      "Collecting pyasn1-modules\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 9.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/valtterikivela/opt/anaconda3/lib/python3.8/site-packages (from zope.interface>=4.1.3->scrapy) (50.3.1.post20201107)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 4.2 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting incremental>=16.10.1\n",
      "  Downloading incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting Automat>=0.8.0\n",
      "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: pycparser in /Users/valtterikivela/opt/anaconda3/lib/python3.8/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.20)\n",
      "Requirement already satisfied: idna>=2.5 in /Users/valtterikivela/opt/anaconda3/lib/python3.8/site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.10)\n",
      "Building wheels for collected packages: protego, PyDispatcher\n",
      "  Building wheel for protego (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for protego: filename=Protego-0.1.16-py3-none-any.whl size=7765 sha256=8014625b9d02dbe0fe2b3af9f89c1e9c77b4b1040a7d6686366cd94d78190fb8\n",
      "  Stored in directory: /Users/valtterikivela/Library/Caches/pip/wheels/91/64/36/bd0d11306cb22a78c7f53d603c7eb74ebb6c211703bc40b686\n",
      "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11515 sha256=31122990221bc5ead9d82f744af3d70cdd23a6ff920be7d99f677bb4ea1b4db6\n",
      "  Stored in directory: /Users/valtterikivela/Library/Caches/pip/wheels/d1/d7/61/11b5b370ee487d38b5408ecb7e0257db9107fa622412cbe2ff\n",
      "Successfully built protego PyDispatcher\n",
      "Installing collected packages: cssselect, w3lib, parsel, jmespath, itemadapter, itemloaders, protego, pyasn1, pyasn1-modules, service-identity, PyDispatcher, hyperlink, constantly, incremental, Automat, Twisted, queuelib, scrapy\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 Twisted-21.2.0 constantly-15.1.0 cssselect-1.1.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.2.0 itemloaders-1.0.4 jmespath-0.10.0 parsel-1.6.0 protego-0.1.16 pyasn1-0.4.8 pyasn1-modules-0.2.8 queuelib-1.5.0 scrapy-2.4.1 service-identity-18.1.0 w3lib-1.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created spider 'kotikokki_scraper' using template 'basic' \n"
     ]
    }
   ],
   "source": [
    "!scrapy genspider kotikokki_scraper kotikokki.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recipe_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-36c6fea4e64b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mKotikokkiScraperSpider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"kotikokki_spider\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mallowed_domains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'kotikokki.net'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-36c6fea4e64b>\u001b[0m in \u001b[0;36mKotikokkiScraperSpider\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mrecipe_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecipe_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'::text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecipe_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'recipe_text' is not defined"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "\n",
    "class KotikokkiScraperSpider(scrapy.Spider):\n",
    "    name = \"kotikokki_spider\"\n",
    "    allowed_domains = ['kotikokki.net']\n",
    "    start_urls = ['https://www.kotikokki.net/reseptit/nayta/855505/Tero-sedän%20Viikinkisima%20%28hunajaviini%20/-kilju%29/']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        recipe_text = response.css('.instructions')\n",
    "        recipe_text = \"\".join(recipe_text.css('::text').extract()).strip()\n",
    "        \n",
    "    print(recipe_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiivis ohje oppimispäiväkirjan kirjoittamiseen:\n",
    "\n",
    "1. Kerro osallistuitko viikon opetukseen ja kuvaa päiväkirjan laatimisessa käyttämäsi aineisto.\n",
    "2. Tiivistä jokaisen luentoviikon keskeiset asiat tekstiksi.\n",
    "3. Kirjaa lisäksi ylös viisi oivallusta tai tärkeintä oppimaasi asiaa erilliseksi listaksi.\n",
    "4. Listaa yhdestä kolmeen kehityskohdetta tai flippausvinkkiä luentoviikolle.\n",
    "5. Kokoa noin kymmenen koodirivin demo viikon aikana oppimastasi."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
